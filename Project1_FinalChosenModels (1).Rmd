---
title: "Project1_FinalChosenModels"
author: "Liam McFall, Cam Farrugia, Erin Karnath"
date: "4/9/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Health Data Set

```{r}

library(readxl)
library(tidyverse)
library(magrittr)
library(boot)
library(randomForest)

health <- read_xlsx("/Users/rachelpeck/Desktop/LiamSchool/DAT514_Proj1/Health.xlsx")

set.seed(7)
rf.cv.fit <- randomForest(X1 ~ X4 + X5, data = health,
                        importance =TRUE)

rf.loocv <- cv.glm(health, rf2.cv.fit)
rf.loocv.error <- rf2.loocv$delta[1]

rf2.loocv.error

```

The best model we came up with was a random forest using X4 and X5 as the predictors. It had a test MSE using LOOCV of 2.11 which was the lowest of all of the models that we tried with the health data set. We choose to use LOOCV when comparing the test errors of our models because it allowed us to use as much of the data in training the model as possible, due to this data set only being 53 observations. We did try using random subset and some k-fold cv as well, but the size of the training sets were too small, and resulted in much larger test errors.